#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script pour importer les montres depuis temps-et-merveilles.fr
T√©l√©charge les images et les classe dans les bonnes cat√©gories
"""

import json
import requests
from bs4 import BeautifulSoup
import os
import re
from urllib.parse import urljoin, urlparse
import time
from pathlib import Path

# Configuration
SOURCE_SITE = "https://temps-et-merveilles.fr"  # Corrigez l'URL si n√©cessaire
PRODUCTS_DIR = "public/images/products"
NEW_PRODUCTS_FILE = "new_products.json"
PLACEHOLDER_IMAGES_FILE = "src/lib/placeholder-images.json"

def clean_filename(name):
    """Nettoie un nom pour en faire un nom de fichier valide"""
    # Convertir en minuscules et remplacer les espaces par des tirets
    name = name.lower()
    # Remplacer les caract√®res sp√©ciaux
    name = re.sub(r'[^a-z0-9\-_]', '-', name)
    # Supprimer les tirets multiples
    name = re.sub(r'-+', '-', name)
    # Supprimer les tirets en d√©but/fin
    name = name.strip('-')
    return name

def download_image(url, filepath):
    """T√©l√©charge une image depuis une URL"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'wb') as f:
            f.write(response.content)
        print(f"  ‚úì Image t√©l√©charg√©e: {filepath}")
        return True
    except Exception as e:
        print(f"  ‚úó Erreur lors du t√©l√©chargement de {url}: {e}")
        return False

def determine_gender(description, name):
    """D√©termine le genre de la montre bas√© sur la description et le nom"""
    text = (description + " " + name).lower()
    
    # Mots-cl√©s pour femmes
    femme_keywords = ['femme', 'woman', 'women', 'ladies', 'dame', 'dames', 'f√©minin', 'feminine', 
                     'rose', 'pink', 'diamant', 'diamond', 'perle', 'pearl', 'd√©licat', 'delicate']
    
    # Mots-cl√©s pour hommes
    homme_keywords = ['homme', 'man', 'men', 'gentleman', 'herren', 'masculin', 'masculine',
                     'sport', 'diver', 'plong√©e', 'aviation', 'pilot', 'militar']
    
    femme_score = sum(1 for keyword in femme_keywords if keyword in text)
    homme_score = sum(1 for keyword in homme_keywords if keyword in text)
    
    if femme_score > homme_score:
        return 'femme'
    elif homme_score > femme_score:
        return 'homme'
    else:
        # Par d√©faut, on met dans accessoires g√©n√©riques
        return 'unisex'

def scrape_products():
    """Scrape les produits depuis le site"""
    print(f"üîç Scraping des produits depuis {SOURCE_SITE}...")
    
    products = []
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # Pour Shopify, essayer l'API JSON
        api_urls = [
            f"{SOURCE_SITE}/products.json",
            f"{SOURCE_SITE}/collections/all/products.json",
        ]
        
        for api_url in api_urls:
            try:
                print(f"  Tentative API: {api_url}")
                response = requests.get(api_url, headers=headers, timeout=30)
                if response.status_code == 200:
                    data = response.json()
                    if 'products' in data:
                        print(f"  ‚úì {len(data['products'])} produits trouv√©s via API")
                        for product_data in data['products']:
                            product = parse_shopify_product(product_data)
                            if product:
                                products.append(product)
                        return products
            except:
                continue
        
        # Si l'API ne fonctionne pas, essayer le scraping HTML
        urls_to_try = [
            f"{SOURCE_SITE}/collections/all",
            f"{SOURCE_SITE}/products",
            f"{SOURCE_SITE}/collections/montres",
            f"{SOURCE_SITE}",
        ]
        
        html_content = None
        for url in urls_to_try:
            try:
                print(f"  Tentative HTML: {url}")
                response = requests.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                html_content = response.text
                print(f"  ‚úì Page r√©cup√©r√©e: {url}")
                break
            except Exception as e:
                print(f"  ‚úó Erreur: {e}")
                continue
        
        if not html_content:
            print("‚ùå Impossible d'acc√©der au site. V√©rifiez l'URL et votre connexion.")
            return products
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Chercher les produits - diff√©rentes structures possibles
        product_elements = soup.find_all(['div', 'article', 'li'], class_=re.compile(r'product|item|card', re.I))
        
        if not product_elements:
            # Essayer de trouver des liens produits
            product_links = soup.find_all('a', href=re.compile(r'/product', re.I))
            print(f"  Trouv√© {len(product_links)} liens produits potentiels")
            
            seen_urls = set()
            for link in product_links:
                href = link.get('href', '')
                if not href or href in seen_urls:
                    continue
                # Nettoyer l'URL
                if href.startswith('/'):
                    href = href
                elif not href.startswith('http'):
                    continue
                
                seen_urls.add(href)
                product_url = urljoin(SOURCE_SITE, href)
                if '/product' in product_url.lower():
                    product = scrape_product_page(product_url)
                    if product and is_valid_product(product.get('name', ''), product_url):
                        products.append(product)
                        print(f"  ‚úì Produit ajout√©: {product.get('name', '')[:50]}")
                        time.sleep(1)  # Pause pour ne pas surcharger le serveur
                    if len(products) >= 200:  # Limiter √† 200 produits
                        break
        else:
            print(f"  Trouv√© {len(product_elements)} √©l√©ments produits")
            seen_urls = set()
            for element in product_elements:
                product = extract_product_from_element(element)
                if product:
                    # √âviter les doublons
                    product_url = product.get('url', '')
                    if product_url and product_url in seen_urls:
                        continue
                    if product_url:
                        seen_urls.add(product_url)
                    products.append(product)
                    if len(products) >= 200:  # Limiter √† 200 produits
                        break
        
    except Exception as e:
        print(f"‚ùå Erreur lors du scraping: {e}")
        import traceback
        traceback.print_exc()
    
    return products

def parse_shopify_product(product_data):
    """Parse un produit depuis l'API Shopify"""
    try:
        name = product_data.get('title', 'Montre')
        
        # Prix
        variants = product_data.get('variants', [])
        price = 0
        if variants:
            price = int(float(variants[0].get('price', 0)) * 100)  # Convertir en centimes puis en euros
        
        # Description
        description = product_data.get('body_html', '')
        # Nettoyer le HTML
        if description:
            soup = BeautifulSoup(description, 'html.parser')
            description = soup.get_text(strip=True)
        
        # Image
        images = product_data.get('images', [])
        image_url = None
        if images:
            image_url = images[0].get('src', '')
            if image_url and not image_url.startswith('http'):
                image_url = 'https:' + image_url
        
        # URL
        handle = product_data.get('handle', '')
        url = f"{SOURCE_SITE}/products/{handle}" if handle else None
        
        # Tags pour d√©terminer le genre
        tags = product_data.get('tags', '')
        gender = determine_gender(description + " " + tags, name)
        
        return {
            'name': name,
            'price': price,
            'image_url': image_url,
            'url': url,
            'description': description,
            'gender': gender
        }
    except Exception as e:
        print(f"  Erreur parsing produit: {e}")
        return None

def is_valid_product(name, url):
    """V√©rifie si c'est un vrai produit (pas un √©l√©ment de navigation)"""
    if not name or len(name.strip()) < 3:
        return False
    
    # Exclure les √©l√©ments de navigation
    excluded_keywords = [
        'acceuil', 'accueil', 'boutique', 'contact', 'blog', '√† propos', 'a propos',
        'information', 'emplacement', 'apprenez', 'notre', 'nous', 'conna√Ætre',
        'panier', 'loading', 'done', 'ajouter', 'produit en vente', '%',
        'üìû', 'phone', 't√©l√©phone', 'tel:', 'mailto:', 'facebook', 'instagram'
    ]
    
    name_lower = name.lower()
    for keyword in excluded_keywords:
        if keyword in name_lower:
            return False
    
    # V√©rifier que l'URL est une vraie page produit
    if url and '/product' in url.lower():
        return True
    
    # Si pas d'URL mais nom valide (au moins 5 caract√®res et contient des lettres)
    if len(name) >= 5 and re.search(r'[a-zA-Z]', name):
        return True
    
    return False

def extract_product_from_element(element):
    """Extrait les informations d'un produit depuis un √©l√©ment HTML"""
    try:
        # Chercher le nom
        name_elem = element.find(['h2', 'h3', 'h4', 'a'], class_=re.compile(r'title|name|product', re.I))
        if not name_elem:
            name_elem = element.find('a')
        name = name_elem.get_text(strip=True) if name_elem else ""
        
        # Chercher le lien
        link_elem = element.find('a', href=True)
        product_url = None
        if link_elem:
            href = link_elem.get('href', '')
            if href and '/product' in href.lower():
                product_url = urljoin(SOURCE_SITE, href)
        
        # V√©rifier si c'est un vrai produit
        if not is_valid_product(name, product_url):
            return None
        
        # Si on a une URL produit, scraper la page compl√®te
        if product_url:
            return scrape_product_page(product_url)
        
        # Sinon, essayer d'extraire depuis l'√©l√©ment
        # Chercher le prix
        price_elem = element.find(['span', 'div'], class_=re.compile(r'price', re.I))
        price_text = price_elem.get_text(strip=True) if price_elem else "0"
        price = extract_price(price_text)
        
        # Chercher l'image
        img_elem = element.find('img')
        image_url = None
        if img_elem:
            image_url = img_elem.get('src') or img_elem.get('data-src') or img_elem.get('data-lazy-src')
            if image_url:
                image_url = urljoin(SOURCE_SITE, image_url)
        
        return {
            'name': name,
            'price': price,
            'image_url': image_url,
            'url': product_url,
            'description': ''
        }
    except Exception as e:
        print(f"  Erreur extraction: {e}")
        return None

def scrape_product_page(url):
    """Scrape une page produit individuelle"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Extraire le nom - chercher dans plusieurs endroits
        name = None
        name_elem = soup.find('h1', class_=re.compile(r'title|name|product', re.I))
        if not name_elem:
            name_elem = soup.find('h1')
        if not name_elem:
            name_elem = soup.find(['h2', 'h3'], class_=re.compile(r'product', re.I))
        if name_elem:
            name = name_elem.get_text(strip=True)
        
        if not name or len(name) < 3:
            return None
        
        # Extraire le prix - chercher dans plusieurs endroits
        price = 0
        price_elem = soup.find(['span', 'div'], class_=re.compile(r'price', re.I))
        if price_elem:
            price_text = price_elem.get_text(strip=True)
            price = extract_price(price_text)
        
        # Si pas de prix trouv√©, chercher dans le texte de la page
        if price == 0:
            price_matches = re.findall(r'(\d+[.,]?\d*)\s*‚Ç¨', soup.get_text())
            if price_matches:
                try:
                    price = int(float(price_matches[0].replace(',', '.')))
                except:
                    pass
        
        # Extraire la description
        description = ""
        desc_elem = soup.find(['div', 'section'], class_=re.compile(r'description|content|details', re.I))
        if desc_elem:
            # Prendre plusieurs paragraphes
            desc_paras = desc_elem.find_all('p')
            if desc_paras:
                description = ' '.join([p.get_text(strip=True) for p in desc_paras[:3]])
            else:
                description = desc_elem.get_text(strip=True)
        
        # Si pas de description, chercher dans meta description
        if not description:
            meta_desc = soup.find('meta', {'name': 'description'})
            if meta_desc:
                description = meta_desc.get('content', '')
        
        # Extraire l'image principale - chercher la meilleure image
        image_url = None
        # Chercher dans les images avec des classes product
        img_elems = soup.find_all('img', class_=re.compile(r'product|main|featured|hero', re.I))
        if not img_elems:
            # Chercher toutes les images et prendre la plus grande
            img_elems = soup.find_all('img')
        
        for img_elem in img_elems:
            src = img_elem.get('src') or img_elem.get('data-src') or img_elem.get('data-lazy-src')
            if src:
                # √âviter les logos et ic√¥nes
                if any(x in src.lower() for x in ['logo', 'icon', 'avatar', 'placeholder']):
                    continue
                image_url = urljoin(SOURCE_SITE, src)
                break
        
        return {
            'name': name,
            'price': price,
            'image_url': image_url,
            'url': url,
            'description': description
        }
    except Exception as e:
        print(f"  Erreur scraping page {url}: {e}")
        return None

def extract_price(price_text):
    """Extrait le prix d'un texte"""
    # Chercher des nombres avec √©ventuellement des d√©cimales
    match = re.search(r'(\d+[.,]?\d*)', price_text.replace(' ', ''))
    if match:
        price_str = match.group(1).replace(',', '.')
        try:
            return int(float(price_str))
        except:
            return 0
    return 0

def create_product_entry(watch_data, index, gender=None):
    """Cr√©e une entr√©e produit au format du site"""
    name = watch_data['name']
    slug = clean_filename(name)
    
    # Utiliser le genre du watch_data si disponible, sinon utiliser celui pass√© en param√®tre
    if 'gender' in watch_data:
        gender = watch_data['gender']
    elif not gender:
        gender = determine_gender(watch_data.get('description', ''), name)
    
    # D√©terminer la cat√©gorie et sous-cat√©gorie
    if gender == 'femme':
        category = 'womens-clothing'
        subcategory = 'accessoires-femme'
        product_id = f'accessoires-femme-{index+1:03d}'
    elif gender == 'homme':
        category = 'mens-clothing'
        subcategory = 'accessoires-homme'
        product_id = f'accessoires-homme-{index+1:03d}'
    else:
        category = 'accessories'
        subcategory = None
        product_id = f'accessoires-{index+1:03d}'
    
    # G√©n√©rer les noms multilingues (basique - √† am√©liorer)
    name_fr = name
    name_en = name  # √Ä traduire si n√©cessaire
    name_de = name  # √Ä traduire si n√©cessaire
    
    # G√©n√©rer les descriptions multilingues
    description = watch_data.get('description', f'Une montre √©l√©gante {name}')
    description_fr = description
    description_en = description  # √Ä traduire si n√©cessaire
    description_de = description  # √Ä traduire si n√©cessaire
    
    # Nom de l'image
    image_id = slug.replace('-', '_')
    
    product = {
        'id': product_id,
        'name': name_de,
        'name_fr': name_fr,
        'name_en': name_en,
        'slug': slug,
        'price': watch_data.get('price', 0),
        'oldPrice': None,
        'description': description_de,
        'description_fr': description_fr,
        'description_en': description_en,
        'category': category,
        'subcategory': subcategory,
        'images': [image_id],
        'sizes': None,  # Les montres n'ont g√©n√©ralement pas de tailles
        'colors': None
    }
    
    return product, image_id, watch_data.get('image_url')

def main():
    print("üöÄ D√©but de l'importation des montres depuis temps-et-merveilles.fr\n")
    
    # Scraper les produits
    watches = scrape_products()
    
    if not watches:
        print("\n‚ùå Aucun produit trouv√©. V√©rifiez l'URL et la structure du site.")
        return
    
    print(f"\n‚úì {len(watches)} produits trouv√©s\n")
    
    # Charger les fichiers existants
    try:
        with open(NEW_PRODUCTS_FILE, 'r', encoding='utf-8') as f:
            existing_products = json.load(f)
    except:
        existing_products = []
    
    try:
        with open(PLACEHOLDER_IMAGES_FILE, 'r', encoding='utf-8') as f:
            placeholder_data = json.load(f)
            placeholder_images = placeholder_data.get('placeholderImages', [])
    except:
        placeholder_images = []
    
    # Traiter chaque montre
    new_products = []
    new_images = []
    
    for i, watch in enumerate(watches):
        print(f"\n[{i+1}/{len(watches)}] Traitement: {watch.get('name', 'Montre')}")
        
        # Le genre peut d√©j√† √™tre dans watch_data si venant de Shopify
        gender = watch.get('gender')
        if not gender:
            gender = determine_gender(watch.get('description', ''), watch.get('name', ''))
        print(f"  Genre d√©tect√©: {gender}")
        
        # Cr√©er l'entr√©e produit
        product, image_id, image_url = create_product_entry(watch, i, gender)
        
        # T√©l√©charger l'image
        if image_url:
            image_filename = f"{image_id}.jpg"
            image_path = os.path.join(PRODUCTS_DIR, image_filename)
            image_relative_path = f"/images/products/{image_filename}"
            
            if download_image(image_url, image_path):
                # Ajouter √† placeholder_images
                new_images.append({
                    'id': image_id,
                    'description': product['name_fr'],
                    'imageUrl': image_relative_path,
                    'imageHint': 'watch'
                })
                print(f"  ‚úì Image ajout√©e: {image_id}")
            else:
                print(f"  ‚ö† Image non t√©l√©charg√©e, utilisation d'un placeholder")
        else:
            print(f"  ‚ö† Aucune image trouv√©e pour ce produit")
        
        new_products.append(product)
    
    # Fusionner avec les produits existants
    all_products = existing_products + new_products
    
    # Sauvegarder
    print(f"\nüíæ Sauvegarde de {len(new_products)} nouveaux produits...")
    with open(NEW_PRODUCTS_FILE, 'w', encoding='utf-8') as f:
        json.dump(all_products, f, ensure_ascii=False, indent=2)
    print(f"‚úì {NEW_PRODUCTS_FILE} mis √† jour")
    
    # Fusionner les images
    all_images = placeholder_images + new_images
    
    print(f"\nüíæ Sauvegarde de {len(new_images)} nouvelles images...")
    with open(PLACEHOLDER_IMAGES_FILE, 'w', encoding='utf-8') as f:
        json.dump({'placeholderImages': all_images}, f, ensure_ascii=False, indent=2)
    print(f"‚úì {PLACEHOLDER_IMAGES_FILE} mis √† jour")
    
    print(f"\n‚úÖ Importation termin√©e!")
    print(f"   - {len(new_products)} produits ajout√©s")
    print(f"   - {len(new_images)} images t√©l√©charg√©es")

if __name__ == '__main__':
    main()

